{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_H0 -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_H0, n_W0, n_C0], name = \"x_in\")\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, n_y], name = \"y_in\")\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    layer_depth = {\n",
    "        'conv1': 6,\n",
    "        'conv2': 16,\n",
    "        'fc1': 120,\n",
    "        'fc2': 84,\n",
    "        'fc3': n_classes\n",
    "    }\n",
    "\n",
    "    filter_size={\n",
    "        'conv_1': 5\n",
    "        ,'conv_2': 5\n",
    "    }\n",
    "    \n",
    "    tf.set_random_seed(1)                             \n",
    "\n",
    "    weights = {\n",
    "        'conv1': tf.get_variable(name=\"conv1w\", shape=[filter_size['conv_1'], filter_size['conv_1'], 3, layer_depth['conv1']], initializer=tf.contrib.layers.xavier_initializer(seed = 0)),\n",
    "        'conv2': tf.get_variable(name=\"conv2w\", shape=[filter_size['conv_2'], filter_size['conv_2'], layer_depth['conv1'], layer_depth['conv2']], initializer=tf.contrib.layers.xavier_initializer(seed = 0)),\n",
    "        'fc1':  tf.get_variable(name=\"fc1w\", shape=[1024, layer_depth['fc1']], initializer=tf.contrib.layers.xavier_initializer(seed = 0)),\n",
    "        'fc2':  tf.get_variable(name=\"fc2w\", shape=[layer_depth['fc1'], layer_depth['fc2']], initializer=tf.contrib.layers.xavier_initializer(seed = 0)),\n",
    "        'fc3':  tf.get_variable(name=\"fc3w\", shape=[layer_depth['fc2'], layer_depth['fc3']], initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'conv1': tf.get_variable(name=\"conv1b\", shape=[layer_depth['conv1']], initializer=tf.contrib.layers.xavier_initializer(seed = 0)),\n",
    "        'conv2': tf.get_variable(name=\"conv2b\", shape=[layer_depth['conv2']], initializer=tf.contrib.layers.xavier_initializer(seed = 0)),\n",
    "        'fc1': tf.get_variable(name=\"fc1b\", shape=[layer_depth['fc1']], initializer=tf.contrib.layers.xavier_initializer(seed = 0)),\n",
    "        'fc2': tf.get_variable(name=\"fc2b\", shape=[layer_depth['fc2']], initializer=tf.contrib.layers.xavier_initializer(seed = 0)),\n",
    "        'fc3': tf.get_variable(name=\"fc3b\", shape=[layer_depth['fc3']], initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    }\n",
    "    \n",
    "    \n",
    "    parameters = {\"weights\": weights,\n",
    "                  \"biases\": biases}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **TensorFlow**, there are built-in functions that carry out the convolution steps.\n",
    "\n",
    "`tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = 'SAME')`: given an input $X$ and a group of filters $W1$, this function convolves $W1$'s filters on X. The third input ([1,f,f,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev).\n",
    "\n",
    "`tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = 'SAME')`: given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window.\n",
    "\n",
    "`tf.nn.relu(Z1)`: computes the elementwise ReLU of Z1 (which can be any shape).\n",
    "\n",
    "`tf.contrib.layers.flatten(P)`: given an input P, this function flattens each example into a 1D vector it while maintaining the batch-size. It returns a flattened tensor with shape [batch_size, k].\n",
    "\n",
    "`tf.contrib.layers.fully_connected(F, num_outputs)`: given a the flattened input F, it returns the output computed using a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    weights = parameters['weights']\n",
    "    biases = parameters['biases']\n",
    "\n",
    "    stride = {\n",
    "        'conv_1': 1\n",
    "        ,'conv_2': 1\n",
    "        ,'maxpool_1': 2\n",
    "        ,'maxpool_2': 2\n",
    "    }\n",
    "    \n",
    "    kernel_size={\n",
    "        'maxpool_1': 2\n",
    "        ,'maxpool_2': 2\n",
    "    }\n",
    "    \n",
    "    # CONV2D: stride of 1, padding 'SAME'\n",
    "    # tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)\n",
    "    Z1 = tf.nn.conv2d(X,weights[\"conv1\"], strides = [1,stride[\"conv_1\"],stride[\"conv_1\"],1]\n",
    "                      , padding = 'SAME'\n",
    "                      , name=\"Conv_1\")\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(Z1, name=\"Relu_1\")\n",
    "    # MAXPOOL: window 8x8, sride 8, padding 'SAME'\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,kernel_size[\"maxpool_1\"],kernel_size[\"maxpool_1\"],1]\n",
    "                        , strides = [1,stride[\"maxpool_1\"],stride[\"maxpool_1\"],1]\n",
    "                        , padding = 'SAME'\n",
    "                        , name=\"Maxpool_1\")\n",
    "    # CONV2D: filters W2, stride 1, padding 'SAME'\n",
    "    Z2 = tf.nn.conv2d(P1,weights[\"conv2\"]\n",
    "                      , strides = [1,stride[\"conv_2\"],stride[\"conv_2\"],1]\n",
    "                      , padding = 'SAME'\n",
    "                      , name=\"Conv_2\")\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(Z2, name=\"Relu_2\")\n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1,kernel_size[\"maxpool_2\"],kernel_size[\"maxpool_2\"],1]\n",
    "                        , strides= [1,stride[\"maxpool_2\"],stride[\"maxpool_2\"],1]\n",
    "                        , padding = 'SAME'\n",
    "                        , name=\"Maxpool_2\")\n",
    "    # FLATTEN\n",
    "    FLAT_2 = tf.contrib.layers.flatten(P2)\n",
    "    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n",
    "    # 43 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n",
    "    FC3 = tf.nn.xw_plus_b(FLAT_2, weights['fc1'], biases['fc1'], name=\"FC_1\")\n",
    "    Z3 = tf.nn.relu(FC3, name=\"Relu_3\")\n",
    "    # FULLY-CONNECTED\n",
    "    FC4 = tf.nn.xw_plus_b(FC3, weights['fc2'], biases['fc2'], name=\"FC_2\")\n",
    "    Z4 = tf.nn.relu(FC4, name=\"Relu_4\")\n",
    "    # FULLY-CONNECTED\n",
    "    FC5 = tf.nn.xw_plus_b(FC4, weights['fc3'], biases['fc3'], name=\"FC_3\")\n",
    "    \n",
    "    print (A1)\n",
    "    print (P1)\n",
    "    print (A2)\n",
    "    print (P2)\n",
    "    print (FLAT_2)\n",
    "    print (FC3)\n",
    "    print (Z3)\n",
    "    print (FC4)\n",
    "    print (Z4)\n",
    "    print (FC5)\n",
    "    \n",
    "#     Z3 = tf.contrib.layers.fully_connected(P2, 43, activation_fn=None)\n",
    "\n",
    "    return FC5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu_1:0\", shape=(?, 32, 32, 6), dtype=float32)\n",
      "Tensor(\"Maxpool_1:0\", shape=(?, 16, 16, 6), dtype=float32)\n",
      "Tensor(\"Relu_2:0\", shape=(?, 16, 16, 16), dtype=float32)\n",
      "Tensor(\"Maxpool_2:0\", shape=(?, 8, 8, 16), dtype=float32)\n",
      "Tensor(\"Flatten/flatten/Reshape:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"FC_1:0\", shape=(?, 120), dtype=float32)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 120), dtype=float32)\n",
      "Tensor(\"FC_2:0\", shape=(?, 84), dtype=float32)\n",
      "Tensor(\"Relu_4:0\", shape=(?, 84), dtype=float32)\n",
      "Tensor(\"FC_3:0\", shape=(?, 43), dtype=float32)\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(32, 32, 3, 4)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    a = sess.run(Z3, {X: np.random.randn(2,32,32,3), Y: np.random.randn(2,4)})\n",
    "    print (Z3.get_shape()[1])\n",
    "#     print(\"Z3 = \" + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    cross_ent = tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y, name=\"cross_entropy\")\n",
    "    cost = tf.reduce_mean(cross_ent, name=\"cost\")\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_batch(X_data, y_data, batch_size=64):\n",
    "    \"\"\"\n",
    "    Creates a list of minibatches from (X, Y) for evaluation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    accuracy -- accuracy of model\n",
    "    \"\"\"\n",
    "    mini_batches = []\n",
    "    m = len(X_data)\n",
    "    num_complete_minibatches = math.ceil(m/batch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X, mini_batch_Y = X_data[k * batch_size:(k + 1) * batch_size], y_data[k * batch_size:(k + 1) * batch_size]\n",
    "        mini_batch = [mini_batch_X, mini_batch_Y]\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.001,\n",
    "          num_epochs = 1, minibatch_size = 64, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer ConvNet in Tensorflow:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_train -- test set, of shape (None, n_y = 6)\n",
    "    X_test -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_test -- test set, of shape (None, n_y = 6)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n",
    "    seed = 3                                          # to keep results consistent (numpy seed)\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    training_accuracies = []\n",
    "    validation_accuracies = []\n",
    "    \n",
    "    \n",
    "    # Create Placeholders of the correct shape\n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z5 = forward_propagation(X, parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z5, Y)\n",
    "\n",
    "    # Calculate the correct predictions\n",
    "    correct_prediction = tf.equal(tf.argmax(Z5, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            minibatch_cost = 0.\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            print (\"epoch: \"+str(epoch))\n",
    "            \n",
    "            for i,(minibatch_X, minibatch_Y) in enumerate(minibatches,1):\n",
    "#                 print (\"batch: \"+str(i)+\"/\"+str(len(minibatches)))\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "                \n",
    "            # Do the evaluation loop\n",
    "            print (\"evaluation start\")\n",
    "            minibatch_test = evaluate_with_batch(X_test, Y_test)\n",
    "            minibatch_train = evaluate_with_batch(X_train, Y_train)\n",
    "\n",
    "            train_accuracy = 0.0\n",
    "            test_accuracy = 0.0\n",
    "\n",
    "            for i,(minibatch_X, minibatch_Y) in enumerate(minibatch_train, 1):\n",
    "    #             print (\"batch: \"+str(i)+\"/\"+str(len(minibatch_train)))\n",
    "                train_accuracy += accuracy.eval({X: minibatch_X, Y: minibatch_Y})\n",
    "\n",
    "            for i,(minibatch_X, minibatch_Y) in enumerate(minibatch_test, 1):\n",
    "    #             print (\"batch: \"+str(i)+\"/\"+str(len(minibatch_test)))\n",
    "                test_accuracy += accuracy.eval({X: minibatch_X, Y: minibatch_Y})\n",
    "            \n",
    "            print (train_accuracy/len(X_train))\n",
    "            print (test_accuracy/len(X_test))\n",
    "            training_accuracies.append(train_accuracy/len(X_train))\n",
    "            validation_accuracies.append(test_accuracy/len(X_test))\n",
    "            \n",
    "            print (\"evaluation ends\")\n",
    "            \n",
    "        print(\"Train Accuracy:\", training_accuracies)\n",
    "        print(\"Test Accuracy:\", validation_accuracies)    \n",
    "\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        save_path = saver.save(sess, os.path.join(\"model\", \"true_Lenet\",\"fake_Lenet.ckpt\"))\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "        \n",
    "        return training_accuracies, validation_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu_1:0\", shape=(?, 32, 32, 6), dtype=float32)\n",
      "Tensor(\"Maxpool_1:0\", shape=(?, 16, 16, 6), dtype=float32)\n",
      "Tensor(\"Relu_2:0\", shape=(?, 16, 16, 16), dtype=float32)\n",
      "Tensor(\"Maxpool_2:0\", shape=(?, 8, 8, 16), dtype=float32)\n",
      "Tensor(\"Flatten/flatten/Reshape:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"FC_1:0\", shape=(?, 120), dtype=float32)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 120), dtype=float32)\n",
      "Tensor(\"FC_2:0\", shape=(?, 84), dtype=float32)\n",
      "Tensor(\"Relu_4:0\", shape=(?, 84), dtype=float32)\n",
      "Tensor(\"FC_3:0\", shape=(?, 43), dtype=float32)\n",
      "epoch: 0\n",
      "Cost after epoch 0: 1.250390\n",
      "evaluation start\n",
      "0.0138262845169\n",
      "0.0126130121635\n",
      "evaluation ends\n",
      "epoch: 1\n",
      "evaluation start\n",
      "0.0146079208787\n",
      "0.0131359224687\n",
      "evaluation ends\n",
      "epoch: 2\n",
      "evaluation start\n",
      "0.0151055830164\n",
      "0.0136185149352\n",
      "evaluation ends\n",
      "epoch: 3\n",
      "evaluation start\n",
      "0.0153299198117\n",
      "0.013838919193\n",
      "evaluation ends\n",
      "epoch: 4\n",
      "evaluation start\n",
      "0.0152729322491\n",
      "0.0137166217071\n",
      "evaluation ends\n",
      "epoch: 5\n",
      "Cost after epoch 5: 0.087421\n",
      "evaluation start\n",
      "0.0153548063154\n",
      "0.0140337888075\n",
      "evaluation ends\n",
      "epoch: 6\n",
      "evaluation start\n",
      "0.0153469123098\n",
      "0.01392028794\n",
      "evaluation ends\n",
      "epoch: 7\n",
      "evaluation start\n",
      "0.0152511712284\n",
      "0.0135869937013\n",
      "evaluation ends\n",
      "epoch: 8\n",
      "evaluation start\n",
      "0.0153906381876\n",
      "0.0139338494023\n",
      "evaluation ends\n",
      "epoch: 9\n",
      "evaluation start\n",
      "0.0154196322934\n",
      "0.0140298791889\n",
      "evaluation ends\n",
      "Train Accuracy: [0.013826284516886538, 0.014607920878710063, 0.015105583016414587, 0.015329919811694374, 0.015272932249066591, 0.015354806315421989, 0.015346912309843969, 0.015251171228415298, 0.015390638187561792, 0.015419632293446178]\n",
      "Test Accuracy: [0.012613012163547162, 0.013135922468708757, 0.013618514935175578, 0.01383891919302562, 0.01371662170708585, 0.01403378880753809, 0.013920287939966941, 0.013586993701333632, 0.013933849402295759, 0.014029879188861977]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf283612c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: model\\true_Lenet\\fake_Lenet.ckpt\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, test_accuracy = model(X_train_norm, labels_train, X_valid_norm, labels_valid, num_epochs = )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though there is no file named model.ckpt, you still refer to the saved checkpoint by that name when restoring it. \n",
    "* `.meta` stores the graph structure. \n",
    "* `.data` stores the values of each variable in the graph. \n",
    "* `.index` identifies the checkpiont. \n",
    "So in the example above: import_meta_graph uses the .meta, and saver.restore uses the .data and .index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model\\true_Lenet\\fake_Lenet.ckpt\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Operation' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-173-570a3f752534>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_operation_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cost\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda_36\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         grad_loss=grad_loss)\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda_36\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[1;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[0;32m    392\u001b[0m                        \u001b[1;34m\"Optimizer.GATE_OP, Optimizer.GATE_GRAPH.  Not %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m                        gate_gradients)\n\u001b[1;32m--> 394\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_valid_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgrad_loss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_valid_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrad_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda_36\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36m_assert_valid_dtypes\u001b[1;34m(self, tensors)\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[0mvalid_dtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_valid_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalid_dtypes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m         raise ValueError(\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Operation' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph(os.path.join(\"model\", \"true_Lenet\",\"fake_Lenet.ckpt.meta\"))\n",
    "    saver.restore(sess, os.path.join(\"model\", \"true_Lenet\",\"fake_Lenet.ckpt\"))\n",
    "    g = tf.get_default_graph()\n",
    "#     for i in g.get_operations():\n",
    "#         print (i.name)\n",
    "\n",
    "    learning_rate = 0.0001\n",
    "    cost = g.get_operation_by_name(\"cost\")\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    for epoch_i in range(1):\n",
    "\n",
    "        minibatch_test = evaluate_with_batch(X_valid_norm, labels_valid)\n",
    "        test_accuracy = 0.0\n",
    "\n",
    "        for i,(minibatch_X, minibatch_Y) in enumerate(minibatch_test, 1):\n",
    "            _, curr_cost = sess.run([optimizer, cost], feed_dict={x_in: batch})\n",
    "            \n",
    "            test_accuracy += accuracy.eval({X: minibatch_X, Y: minibatch_Y})\n",
    "        \n",
    "    save_path = tf.train.Saver().save(sess, os.path.join(\"model\", \"true_Lenet\",\"fake_Lenet.ckpt\"))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_model(X_train, Y_train, X_test, Y_test, learning_rate = 0.001, num_epochs = 1, minibatch_size = 64, print_cost = True):\n",
    "\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    training_accuracies = []\n",
    "    validation_accuracies = []\n",
    "    \n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph(os.path.join(\"model\", \"true_Lenet\",\"fake_Lenet.ckpt.meta\"))\n",
    "        saver.restore(sess, os.path.join(\"model\", \"true_Lenet\",\"fake_Lenet.ckpt\"))\n",
    "        \n",
    "        \n",
    "        g = tf.get_default_graph()\n",
    "        x_in = g.get_tensor_by_name(\"x_in:0\")\n",
    "        cost = g.get_tensor_by_name(\"cost:0\")\n",
    "\n",
    "#         # Run the initialization\n",
    "#         sess.run(init)\n",
    "\n",
    "#         m = len(X_train)\n",
    "#         minibatch_size = 64\n",
    "#         num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "#         num_epochs = 10\n",
    "#         seed = 0\n",
    "#         # Do the training loop\n",
    "#         for epoch in range(num_epochs):\n",
    "\n",
    "#             minibatch_cost = 0.\n",
    "#             seed = seed + 1\n",
    "#             minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "#             print (\"epoch: \"+str(epoch))\n",
    "\n",
    "#             for i,(minibatch_X, minibatch_Y) in enumerate(minibatches,1):\n",
    "#     #                 print (\"batch: \"+str(i)+\"/\"+str(len(minibatches)))\n",
    "#                 # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "#                 # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "#                 _ , temp_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "\n",
    "#                 minibatch_cost += temp_cost / num_minibatches\n",
    "\n",
    "\n",
    "#             # Print the cost every epoch\n",
    "#             if print_cost == True and epoch % 1 == 0:\n",
    "#                 print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "#             if print_cost == True and epoch % 1 == 0:\n",
    "#                 costs.append(minibatch_cost)\n",
    "\n",
    "#         # plot the cost\n",
    "#         plt.plot(np.squeeze(costs))\n",
    "#         plt.ylabel('cost')\n",
    "#         plt.xlabel('iterations (per tens)')\n",
    "#         plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "#         plt.show()\n",
    "\n",
    "#         saver.save(sess, os.path.join(\"model\", \"true_Lenet\",\"fake_Lenet.ckpt\"))\n",
    "#         print(\"Model saved\")\n",
    "        \n",
    "#         return training_accuracies, validation_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-11624a63064e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresume_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valid_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-140-caacbdb8acc3>\u001b[0m in \u001b[0;36mresume_model\u001b[1;34m(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs, minibatch_size, print_cost)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Initialize all the variables globally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cost' is not defined"
     ]
    }
   ],
   "source": [
    "train_accuracy, test_accuracy = resume_model(X_train_norm, labels_train, X_valid_norm, labels_valid, num_epochs = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
