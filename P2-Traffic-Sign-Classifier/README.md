## Building a Traffic Sign Classifier

![traffic signs](img/traffic_signs.png)

Overview
---
In this project, deep learning techniques and convolutional neural networks were applied to classify traffic signs. The traffic sign images are from [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset). After training, the images of the traffic signs from the Internet were incorporated to test the performance on the model .

Files
---
The structure and useage of the files in this repository are as follows:

1. `Main_pipeline.ipynb` and `Main_pipeline.html`
  - This part mainly contains the data exploration, visualization, preprocessing, model architecture and evaluation on test images.

2. `model_ec2.ipynb`
  - Most part of this file are the same the previous one. The differences are the code and training procedure. This file was run on AWS EC2 with GPU, so more parameter tuning were tried on the files. Also, some code change on Keras was performed to solve the compatibility issue. Some details could be found in the `Readme.md` of another depository [test_on_AWS_EC2](https://github.com/YouYueHuang/test_on_AWS_EC2)

3. `model_reshuffled_data.ipynb`
  - Most part of this file are the same the previous one. The only difference is the data. The training, test and validation sets are under mixing, shuffling and partitioning to recreate new datasets.

4. `download_file.py`
  - This file contains the functions to download the data because the original data size is over 100 MB and it could not be uploaded to Github.

5. `test_images`
  - It contains 10 images from the Internet for testing, and the file name is its class. Each image are resized to 32*32 already.
  - In web_crawler, there are 5 directories and test images with the file name of its class. They were crawled with [icrawler](https://pypi.python.org/pypi/icrawler), and the image size are not a fixed number.

6. `model`
  - It contains the weight of trained model. All are in the directory of `EC2_model`

7. `img`
  - It stores the images during analysis

8. `test`
  - It contains some functions for preprocessing (grayscale, contrast-limited adaptive histogram equalization, histogram of oriented gradients, data augmentation, data shuffeling and partitioning, stratified sampling, Xavier weight initialization), visualization(tensorboard, graphviz), CNN model framework(inception, devolution net). 

9. `ref`
  - It stores the papers of related research work.

10. `logs`
  - It stores the summary of model training log in previous models, and it is mainly for visualization with TensorBoard. It is abandoned for this moment.

[//]: # (Image References)

[image10]: ./img/traffic_signs.png "traffic_signs"
[image2]: ./img/dataset_histogram.png "The Histogram of training set distribution"
[image9]: ./img/distribution_of_class_frequency.png "Training Dataset probability distribution"
[image1]: ./img/dataset_samples.png "Dataset samples visualization"
[image3]: ./img/confusion_matrix.png "Confustion matrix of validation dataset"

[image4]: ./img/top5_prediction_1.png "top5 prediction of test Samples 1"
[image5]: ./img/top5_prediction_2.png "top5 prediction of test Samples 2"
[image6]: ./img/inception_layer_featuremaps.png "Featuremaps visualization of Inception layer 1"
[image7]: ./img/inception_block.png "Inception Block"
[image8]: ./img/Network_model.png "Network Model"

Data exploration
---
1. Dataset Summary

  * The training data set consists of 43 classes of colored traffic sign images 32x32 in size.
  * The number of training set: 34799.
  * The number of validation set: 4410.
  * The number of test set: 12630.

2. Exploratory Visualization

The following figure shows sample image of each class.
![alt text][image10]

Normalized frequencies of each class in the training, validation and test set are shown below. It shows that the original datasets were sampled evenly for each class, so the these three sets follows the same distribution. 
![alt text][image9]

Next, We plotted the histogram of Training data to give intuition on the frequency distrpution.
![alt text][image2]

Data Pipeline
---
1. Preprocessing

  - [Data augmentation](https://github.com/vxy10/ImageAugmentation): Due to limited data and the class imbalance, additional data was generated by affine transformation including: 
    (1) rotation with random number generated between +/- 15 degress 
    (2) translation by +/- 10 pixels along vertical and horizontal direction
    (3) shearing

    After data augmentation, the size of of each class in training set is at least 600. The size of training set becomes 41469.

The example of augmented data is as follows
<table>
  <tr>
    <td>Origin image</td>
    <td>Affine Transformed images</td>
  </tr> 
  <tr>
    <td><img src='./img/sample_traffic_sign.jpg' style='width: 500px;'></td>
    <td><img src='./img/dataAugment.png' style='width: 500px;'></td>
  </tr>
</table>

  - Image normalization: The RGB value of a image is divided by 255, and the range is between 0~1.
  - Dataset shuffling: It is applied to increase the robustness.

2. Model architecture

  - Composition: There are three types of component in this architecture (1)conv-L2-batchnorm-relu (2)maxpool-dropout (3) fully connection-relu-dropout. Pooling reduces signal and makes the model more robust against spatial invariance. The exact amount of maxpooling will make the model work fine and reduce the parameter. Fully connected layer are applied at the end to classify the images to their classes. 

  - Regularization: The combination of L2 and dropout will act as a regularizer, preventing overfitting and keeping the weights small so that the model is able to generalize pretty well.

  - Batch Normalizaion: It is applied for preventing the interaction of each layer and the saturation of the filters after activation.

The training model is built using keras containing layers as described below:

```python 

    # 1: convolutional layer: kernel size 3x3 (input), L2 regularization with weight_decay 1e-4
    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=X_train.shape))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    
    # 2: maxpool layer: kernel size 2x2
    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))
    model.add(Dropout(0.2))

    # 3: convolutional layer: kernel size 3x3, L2 regularization with weight_decay 1e-4
    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))

    # 4: maxpool layer: kernel size 2x2
    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))
    model.add(Dropout(0.2))
    
    # 5: convolutional layer: kernel size 3x3, L2 regularization with weight_decay 1e-4
    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    
    # 6: maxpool layer: kernel size 2x2
    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))
    model.add(Dropout(0.3))
    
    model.add(Flatten())
    
    # 7: fully-connected layter: 256 neurons
    model.add(Dense(256))
    model.add(Activation('relu'))
    model.add(Dropout(0.3))
    
    # 8: fully-connected layter: 128 neurons
    model.add(Dense(128))
    model.add(Activation('relu'))
    model.add(Dropout(0.1))
    
    # 9: fully-connected layter: 43 neurons with softmax function (output)
    model.add(Dense(43))
    model.add(Activation('softmax'))

    # total parameters: 427,755
    # model size: 1,718 KB
```

3. Early stopping
  - It is applied for regularization to avoid overfitting. The accuracy of test set will increase and then decrease due to the overfitting. Therefore, if the accuracy achieve 93%, the training will be stopped.

4. Evaluation
  - There is no golden rule for a good validation frequency. Validation error was computed after each epoch.

Result
---

The following figure shows the performance in each class.
![alt text][image3]

There are 5 parts with relatively low performace:
(1) 58 images of class 4 and 27 images of class 8 were misclassied as class 19
(2) 26 images of class 7 were misclassied as class 5
(3) 16 images of class 31 were misclassied as class 23
(4) 19 images of class 16 were misclassied as class 41


Conclusion
---

* Model architecture

Without the dropout, the accuracy on training set can be as high as over 99%, but it causes overfitting and leads to low accuracy on other sets. Without maxpool layer, the space use could be as high as 1 GB. 

* Test on images from the Internet

When image is well processed and selected, the accuracy on test images could almost achieve accuracy of 100%, but the signs could not be recognized if we used the test images from the Internet. One of the reasons is the issue of size. The image size and ratio from the Internet are quite diverse. Without the aid of object localization, there might be many noises on the image (e.g, car, electric pole, building, etc.) After resizing, the images are seriously distorted, which makes the key features harder to be detected in the model.

* The accuracy and loss

The model can achieve 99% accuracy on training set without regularization techniques(e.g, dropout, L2), but the accuracy on test set and validation set are about 93% and 93.5% respectively. The loss on training, test and validation set are 0.360, 0.401 and 0.057. The high accuracy on the training set but low accuracy on the validation and test set implies overfitting. The change of L2 just made the model more underfitting on both training set and validation set, so the fine-funning works are done on the adjustment of keeping probability. The policy of tuning was to reduce the difference of loss on training set and validation set while achieving 93% accuracy.

* The issues in preprocessing

Due to the class imbalance, a data augmentation algorithm found on Github was applied to compensate the lack of traffic signs of some categories. The algorithm calculates the number of a class and decided if the image of that class needs to be generated. If the number is less than the threshold, the algorithm will generate 10 images and append them to the origin image. According to the result of error analysis, there was a trend that the misclassified images occured in series. After checking the misclassified images, it turns out that the preprocessing could not enhance the contrast of the dark images well (some are even worse) so that the number of wrongly processed images were generated. If preprocessed images owns some features similar to other classes, that might confuse the model and misclass the image.



### Refrences

1. [Example of Tensorboard](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/4_Utils/tensorboard_basic.py)
2. [Example of saving and restoring model in Tensorflow](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/4_Utils/save_restore_model.py)
3. [The code of Data augmentation](https://github.com/aleju/imgaug) from aleje.
4. [Understanding data augmentation for classification:when to warp?](https://arxiv.org/pdf/1609.08764.pdf)

The following figure shows some sample images of each class to visualize the general view.
![alt text][image1]