{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy import stats\n",
    "import os\n",
    "from os.path import join, basename\n",
    "from collections import deque\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "import imageio \n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.plugins.ffmpeg.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img):\n",
    "    imshape = img.shape\n",
    "\n",
    "    gray_img = grayscale(img) # output an image with only one color channel\n",
    "\n",
    "    low_threshold = 0\n",
    "    high_threshold = 200\n",
    "    canny_img = canny(gray_img, low_threshold, high_threshold)\n",
    "\n",
    "    kernel_size = 5\n",
    "    blurred_img = gaussian_blur(canny_img, kernel_size)\n",
    "    vertices = np.array([[(0,imshape[0]),(imshape[1]//2, imshape[0]//2), (imshape[1],imshape[0])]], dtype=np.int32)\n",
    "    masked_image = region_of_interest(blurred_img, vertices)\n",
    "    for i in [(0,imshape[0]),(imshape[1]//2, imshape[0]//2), (imshape[1],imshape[0])]:\n",
    "        draw_circle(masked_image, i, 20, (255,0,0), thickness = 5)\n",
    "#     plt.imshow(masked_image)\n",
    "    \n",
    "    rho = 1\n",
    "    theta = np.pi/180\n",
    "    threshold = 15\n",
    "    min_line_len = 40\n",
    "    max_line_gap = 5\n",
    "    filter_y_uppderbound = imshape[0]\n",
    "    line_img = hough_lines(masked_image, rho, theta, threshold, min_line_len, max_line_gap, filter_y_uppderbound) \n",
    "    draw_triangle(img, vertices, (0,255,0), 10)\n",
    "\n",
    "    return weighted_img(img, line_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: test_images\\solidWhiteCurve.jpg\n",
      "Processing image: test_images\\solidWhiteRight.jpg\n",
      "Processing image: test_images\\solidYellowCurve.jpg\n",
      "Processing image: test_images\\solidYellowCurve2.jpg\n",
      "Processing image: test_images\\solidYellowLeft.jpg\n",
      "Processing image: test_images\\solidYellowLeft_1.jpg\n",
      "Processing image: test_images\\solidYellowLeft_109.jpg\n",
      "Processing image: test_images\\solidYellowLeft_110.jpg\n",
      "Processing image: test_images\\solidYellowLeft_114.jpg\n",
      "Processing image: test_images\\solidYellowLeft_4.jpg\n",
      "Processing image: test_images\\solidYellowLeft_58.jpg\n",
      "Processing image: test_images\\solidYellowLeft_64.jpg\n",
      "Processing image: test_images\\solidYellowLeft_86.jpg\n",
      "Processing image: test_images\\solidYellowLeft_88.jpg\n",
      "Processing image: test_images\\solidYellowLeft_89.jpg\n",
      "Processing image: test_images\\solidYellowLeft_90.jpg\n",
      "Processing image: test_images\\solidYellowLeft_92.jpg\n",
      "Processing image: test_images\\solidYellowLeft_93.jpg\n",
      "Processing image: test_images\\solidYellowLeft_94.jpg\n",
      "Processing image: test_images\\solidYellowLeft_98.jpg\n",
      "Processing image: test_images\\whiteCarLaneSwitch.jpg\n",
      "2.9190614223480225\n"
     ]
    }
   ],
   "source": [
    "tStart = time.time()\n",
    "# test on images\n",
    "test_images_dir = 'test_images'\n",
    "test_images = [join(test_images_dir, name) for name in os.listdir(test_images_dir)]\n",
    "\n",
    "for test_img in test_images:\n",
    "\n",
    "    print('Processing image: {}'.format(test_img))\n",
    "    \n",
    "    out_path = join('test_images_output', basename(test_img))\n",
    "    in_image = cv2.cvtColor(cv2.imread(test_img, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "    out_image = process_image(in_image)\n",
    "#     Output_error_img(out_path, cv2.cvtColor(out_image, cv2.COLOR_RGB2BGR))\n",
    "    Output_error_img(out_path, out_image)\n",
    "# plt.close('all')\n",
    "tEnd = time.time()\n",
    "print (tEnd - tStart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output error image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Output_img(out_path, img):\n",
    "    return cv2.imwrite(out_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "dir(helper)\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_videos\\\\challenge.mp4']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on videos challenge\n",
    "test_videos_dir = 'test_videos'\n",
    "test_videos = [join(test_videos_dir, name) for name in os.listdir(test_videos_dir)]\n",
    "test_videos = [test_videos[0]]\n",
    "test_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: test_videos\\challenge.mp4\n",
      "the: 1 image\n",
      "the: 2 image\n",
      "the: 3 image\n",
      "the: 4 image\n",
      "the: 5 image\n",
      "the: 6 image\n",
      "the: 7 image\n",
      "the: 8 image\n",
      "the: 9 image\n",
      "the: 10 image\n",
      "the: 11 image\n",
      "the: 12 image\n",
      "the: 13 image\n",
      "the: 14 image\n",
      "the: 15 image\n",
      "the: 16 image\n",
      "the: 17 image\n",
      "the: 18 image\n",
      "the: 19 image\n",
      "the: 20 image\n",
      "the: 21 image\n",
      "the: 22 image\n",
      "the: 23 image\n",
      "the: 24 image\n",
      "the: 25 image\n",
      "the: 26 image\n",
      "the: 27 image\n",
      "the: 28 image\n",
      "the: 29 image\n",
      "the: 30 image\n",
      "the: 31 image\n",
      "the: 32 image\n",
      "the: 33 image\n",
      "the: 34 image\n",
      "the: 35 image\n",
      "the: 36 image\n",
      "the: 37 image\n",
      "the: 38 image\n",
      "the: 39 image\n",
      "the: 40 image\n",
      "the: 41 image\n",
      "the: 42 image\n",
      "the: 43 image\n",
      "the: 44 image\n",
      "the: 45 image\n",
      "the: 46 image\n",
      "the: 47 image\n",
      "the: 48 image\n",
      "the: 49 image\n",
      "the: 50 image\n",
      "the: 51 image\n",
      "the: 52 image\n",
      "the: 53 image\n",
      "the: 54 image\n",
      "the: 55 image\n",
      "the: 56 image\n",
      "the: 57 image\n",
      "the: 58 image\n",
      "the: 59 image\n",
      "the: 60 image\n",
      "the: 61 image\n",
      "the: 62 image\n",
      "the: 63 image\n",
      "the: 64 image\n",
      "the: 65 image\n",
      "the: 66 image\n",
      "the: 67 image\n",
      "the: 68 image\n",
      "the: 69 image\n",
      "the: 70 image\n",
      "the: 71 image\n",
      "the: 72 image\n",
      "the: 73 image\n",
      "the: 74 image\n",
      "the: 75 image\n",
      "the: 76 image\n",
      "the: 77 image\n",
      "the: 78 image\n",
      "the: 79 image\n",
      "the: 80 image\n",
      "the: 81 image\n",
      "the: 82 image\n",
      "the: 83 image\n",
      "the: 84 image\n",
      "the: 85 image\n",
      "the: 86 image\n",
      "the: 87 image\n",
      "the: 88 image\n",
      "the: 89 image\n",
      "the: 90 image\n",
      "the: 91 image\n",
      "the: 92 image\n",
      "the: 93 image\n",
      "the: 94 image\n",
      "the: 95 image\n",
      "the: 96 image\n",
      "the: 97 image\n",
      "the: 98 image\n",
      "the: 99 image\n",
      "the: 100 image\n",
      "the: 101 image\n",
      "the: 102 image\n",
      "the: 103 image\n",
      "the: 104 image\n",
      "the: 105 image\n",
      "the: 106 image\n",
      "the: 107 image\n",
      "the: 108 image\n",
      "the: 109 image\n",
      "the: 110 image\n",
      "the: 111 image\n",
      "the: 112 image\n",
      "the: 113 image\n",
      "the: 114 image\n",
      "the: 115 image\n",
      "the: 116 image\n",
      "the: 117 image\n",
      "the: 118 image\n",
      "the: 119 image\n",
      "the: 120 image\n",
      "the: 121 image\n",
      "the: 122 image\n",
      "the: 123 image\n",
      "the: 124 image\n",
      "the: 125 image\n",
      "the: 126 image\n",
      "the: 127 image\n",
      "the: 128 image\n",
      "the: 129 image\n",
      "the: 130 image\n",
      "the: 131 image\n",
      "the: 132 image\n",
      "the: 133 image\n",
      "the: 134 image\n",
      "the: 135 image\n",
      "the: 136 image\n",
      "the: 137 image\n",
      "the: 138 image\n",
      "the: 139 image\n",
      "the: 140 image\n",
      "the: 141 image\n",
      "the: 142 image\n",
      "the: 143 image\n",
      "the: 144 image\n",
      "the: 145 image\n",
      "the: 146 image\n",
      "the: 147 image\n",
      "the: 148 image\n",
      "the: 149 image\n",
      "the: 150 image\n",
      "the: 151 image\n",
      "the: 152 image\n",
      "the: 153 image\n",
      "the: 154 image\n",
      "the: 155 image\n",
      "the: 156 image\n",
      "the: 157 image\n",
      "the: 158 image\n",
      "the: 159 image\n",
      "the: 160 image\n",
      "the: 161 image\n",
      "the: 162 image\n",
      "the: 163 image\n",
      "the: 164 image\n",
      "the: 165 image\n",
      "the: 166 image\n",
      "the: 167 image\n",
      "the: 168 image\n",
      "the: 169 image\n",
      "the: 170 image\n",
      "the: 171 image\n",
      "the: 172 image\n",
      "the: 173 image\n",
      "the: 174 image\n",
      "the: 175 image\n",
      "the: 176 image\n",
      "the: 177 image\n",
      "the: 178 image\n",
      "the: 179 image\n",
      "the: 180 image\n",
      "the: 181 image\n",
      "the: 182 image\n",
      "the: 183 image\n",
      "the: 184 image\n",
      "the: 185 image\n",
      "the: 186 image\n",
      "the: 187 image\n",
      "the: 188 image\n",
      "the: 189 image\n",
      "the: 190 image\n",
      "the: 191 image\n",
      "the: 192 image\n",
      "the: 193 image\n",
      "the: 194 image\n",
      "the: 195 image\n",
      "the: 196 image\n",
      "the: 197 image\n",
      "the: 198 image\n",
      "the: 199 image\n",
      "the: 200 image\n",
      "the: 201 image\n",
      "the: 202 image\n",
      "the: 203 image\n",
      "the: 204 image\n",
      "the: 205 image\n",
      "the: 206 image\n",
      "the: 207 image\n",
      "the: 208 image\n",
      "the: 209 image\n",
      "the: 210 image\n",
      "the: 211 image\n",
      "the: 212 image\n",
      "the: 213 image\n",
      "the: 214 image\n",
      "the: 215 image\n",
      "the: 216 image\n",
      "the: 217 image\n",
      "the: 218 image\n",
      "the: 219 image\n",
      "the: 220 image\n",
      "the: 221 image\n",
      "the: 222 image\n",
      "the: 223 image\n",
      "the: 224 image\n",
      "the: 225 image\n",
      "the: 226 image\n",
      "the: 227 image\n",
      "the: 228 image\n",
      "the: 229 image\n",
      "the: 230 image\n",
      "the: 231 image\n",
      "the: 232 image\n",
      "the: 233 image\n",
      "the: 234 image\n",
      "the: 235 image\n",
      "the: 236 image\n",
      "the: 237 image\n",
      "the: 238 image\n",
      "the: 239 image\n",
      "the: 240 image\n",
      "the: 241 image\n",
      "the: 242 image\n",
      "the: 243 image\n",
      "the: 244 image\n",
      "transform ends\n"
     ]
    }
   ],
   "source": [
    "resize_h, resize_w = 540, 960\n",
    "for test_video in test_videos:\n",
    "    print('Processing video: {}'.format(test_video))\n",
    "    cap = cv2.VideoCapture(test_video)\n",
    "    in_image = None\n",
    "    a = 0\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "\n",
    "            # cap.read() returns a bool (True/False). If frame is read correctly, it will be True. End of the video can be checked with this return value.\n",
    "            ret, color_frame = cap.read()\n",
    "            if ret:\n",
    "                a+=1; print (\"the: {} image\".format(a))\n",
    "                                \n",
    "                in_image = cv2.cvtColor(color_frame, cv2.COLOR_BGR2RGB)\n",
    "                in_image_cp = np.copy(in_image)\n",
    "                \n",
    "                out_image = process_image(in_image_cp)\n",
    "                \n",
    "                correct_out_path = join('test_images_from_video', 'solidYellowLeft_' + str(a) + \".jpg\")\n",
    "                Output_error_img(correct_out_path, cv2.cvtColor(out_image, cv2.COLOR_RGB2BGR)) \n",
    "                \n",
    "            else:\n",
    "                break\n",
    "        print (\"transform ends\")\n",
    "#     except ValueError as e:    \n",
    "#         error_out_path = join('test_images_from_video_error_output', 'solidYellowLeft_' + str(a) + \".jpg\")\n",
    "#         Output_img(error_out_path, in_image) \n",
    "#         print (\"ValueError:\" + str(e))\n",
    "\n",
    "#     except IndexError as e:\n",
    "#         error_out_path = join('test_images_from_video_error_output', 'solidYellowLeft_' + str(a) + \".jpg\")\n",
    "#         Output_img(error_out_path, in_image) \n",
    "#         print (\"IndexError:\" + str(e))\n",
    "    finally:\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap, filter_y_uppderbound):\n",
    "    lines = np.squeeze(cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap), axis=1)\n",
    "    pos_SlopeLines, neg_SlopeLines, left_center, right_center = hough_line_filter(lines.T, filter_y_uppderbound//2)\n",
    "#     print (pos_SlopeLines.shape)\n",
    "#     draw_lines(img, pos_SlopeLines.T)\n",
    "#     draw_lines(img, neg_SlopeLines.T)\n",
    "#     plt.imshow(img)\n",
    "    \n",
    "    Lane_Y_lowerbound = min(left_center[1], right_center[1])\n",
    "    Lane_Y_upperbound = filter_y_uppderbound\n",
    "    \n",
    "    left_x = np.concatenate((neg_SlopeLines[0], neg_SlopeLines[2]), axis=0)\n",
    "    left_y = np.concatenate((neg_SlopeLines[1], neg_SlopeLines[3]), axis=0)\n",
    "    \n",
    "    right_x = np.concatenate((pos_SlopeLines[0], pos_SlopeLines[2]), axis=0)\n",
    "    right_y = np.concatenate((pos_SlopeLines[1], pos_SlopeLines[3]), axis=0)\n",
    "    \n",
    "    left_slope, left_intercept = get_fitLineCharacter(left_x, left_y)\n",
    "    right_slope, right_intercept = get_fitLineCharacter(right_x, right_y)\n",
    "    \n",
    "    left_Lane_X_upperbound, left_Lane_X_lowerbound, right_Lane_X_upperbound, right_Lane_X_lowerbound = get_fitLane(left_slope, left_intercept, right_slope, right_intercept\n",
    "                                                                                                                    , Lane_Y_lowerbound, Lane_Y_upperbound)\n",
    "    if left_Lane_X_upperbound > right_Lane_X_lowerbound:\n",
    "        left_Lane_X_upperbound = (right_intercept - left_intercept)/(left_slope - right_slope)\n",
    "        right_Lane_X_lowerbound = left_Lane_X_upperbound\n",
    "        Lane_Y_lowerbound = left_Lane_X_upperbound * left_slope + left_intercept\n",
    "    \n",
    "    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "#     draw_lines(line_img, [[left_Lane_X_upperbound, Lane_Y_lowerbound, left_Lane_X_lowerbound, Lane_Y_upperbound]\n",
    "#                           ,[right_Lane_X_upperbound, Lane_Y_upperbound, right_Lane_X_lowerbound, Lane_Y_lowerbound]])\n",
    "    draw_lines(line_img, pos_SlopeLines.T)\n",
    "    draw_lines(line_img, neg_SlopeLines.T)\n",
    "    return line_img # line_img\n",
    "\n",
    "def hough_line_filter(lines, filter_y_uppderbound):\n",
    "#     print (lines)\n",
    "    lines = lines[:,lines[1] > filter_y_uppderbound]\n",
    "    \n",
    "    # calculate the slope of the lines\n",
    "    slopes = np.divide(lines[1]-lines[3], lines[0]-lines[2])\n",
    "#     print (slopes)\n",
    "    \n",
    "    # right lines: get the most top left point\n",
    "    right_lines = (lines[:,np.array((slopes > 0.5))])\n",
    "    right_ref_x, right_ref_y = 0, 0\n",
    "    \n",
    "    if right_lines.shape[1] == 0:\n",
    "        right_ref_x, right_ref_y, _, _ = lines[:,np.argmax(slopes)]\n",
    "        right_lines = lines[:,np.argmax(slopes)].reshape(4,1)    \n",
    "    else:\n",
    "        right_ref_x, right_ref_y, _, _ = right_lines[:,0]\n",
    "    \n",
    "    # left lines: get the most top right point\n",
    "    left_lines = (lines[:,np.array((slopes < -0.5))])\n",
    "    left_ref_x, left_ref_y = 0, 0\n",
    "    \n",
    "    if left_lines.shape[1] == 0:\n",
    "        _, _, left_ref_x, left_ref_y = lines[:,np.argmin(slopes)]\n",
    "        left_lines = lines[:,np.argmin(slopes)].reshape(4,1)    \n",
    "    else:\n",
    "        left_max_x_arg = np.argmax(left_lines[2], axis=0)\n",
    "        _, _, left_ref_x, left_ref_y = left_lines[:,left_max_x_arg]\n",
    "        \n",
    "    return right_lines, left_lines, (left_ref_x, left_ref_y), (right_ref_x, right_ref_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "def canny(img, low_threshold, high_threshold):\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    mask = np.zeros_like(img)   \n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "def weighted_img(initial_img, img, α=1.0, β=0.8, λ=0.):\n",
    "    \"\"\"\n",
    "    initial_img * α + img * β + λ\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, α, img, β, λ)\n",
    "\n",
    "def get_fitLane(left_slope, left_intercept, right_slope, right_intercept, Y_lowerbound, Y_upperbound):\n",
    "    return ((Y_lowerbound - left_intercept)/left_slope\n",
    "            , (Y_upperbound - left_intercept)/left_slope\n",
    "            , (Y_upperbound - right_intercept)/right_slope\n",
    "            , (Y_lowerbound - right_intercept)/right_slope)\n",
    "    \n",
    "def get_fitLineCharacter(x, y):\n",
    "    \"\"\"\n",
    "    format is y = slope * x + intercept\n",
    "    \"\"\"\n",
    "    slope, intercept, _, _, std_err = stats.linregress(x,y)\n",
    "    return slope, intercept\n",
    "\n",
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=10):\n",
    "    for x1, y1, x2, y2 in lines:\n",
    "        cv2.line(img, (int(x1), int(y1)), (int(x2), int(y2)), color, thickness)\n",
    "\n",
    "def draw_circle(img, center, radius, color, thickness = 5):\n",
    "    return cv2.circle(img, center, radius, color, thickness)\n",
    "\n",
    "def draw_triangle(img, vertex, color, thickness = 5):\n",
    "    cv2.polylines(img, [vertex], True, color,thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_image = cv2.cvtColor(cv2.imread(\"test_images\\\\solidWhiteCurve.jpg\", cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "D:\\Build\\OpenCV\\opencv-3.3.1\\modules\\imgproc\\src\\drawing.cpp:2432: error: (-215) p.checkVector(2, 4) >= 0 in function cv::polylines\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-380-d3dd2ceaf182>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdraw_triangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-378-67a044a6dd0b>\u001b[0m in \u001b[0;36mdraw_triangle\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdraw_triangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mvrx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolylines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvrx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31merror\u001b[0m: D:\\Build\\OpenCV\\opencv-3.3.1\\modules\\imgproc\\src\\drawing.cpp:2432: error: (-215) p.checkVector(2, 4) >= 0 in function cv::polylines\n"
     ]
    }
   ],
   "source": [
    "draw_triangle(in_image)\n",
    "plt.imshow(in_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoSaved_dir = \"test_videos_output\"\n",
    "if not os.path.exists(videoSaved_dir):\n",
    "    os.makedirs(videoSaved_dir)\n",
    "\n",
    "\n",
    "white_output = videoSaved_dir + '/solidWhiteRight.mp4'\n",
    "\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "# white_output.subclip(0,2)\n",
    "\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "clip1 = VideoFileClip(\"test_videos//solidWhiteRight.mp4\").subclip(0,5)\n",
    "# clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "# %time white_clip.write_videofile(white_output, fps=10, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the video inline, or if you prefer find the video in your filesystem (should be in the same directory) and play it in your video player of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"test_videos_output/solidWhiteRight.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the draw_lines() function\n",
    "\n",
    "**At this point, if you were successful with making the pipeline and tuning parameters, you probably have the Hough line segments drawn onto the road, but what about identifying the full extent of the lane and marking it clearly as in the example video (P1_example.mp4)?  Think about defining a line to run the full length of the visible lane based on the line segments you identified with the Hough Transform. As mentioned previously, try to average and/or extrapolate the line segments you've detected to map out the full extent of the lane lines. You can see an example of the result you're going for in the video \"P1_example.mp4\".**\n",
    "\n",
    "**Go back and modify your draw_lines function accordingly and try re-running your pipeline. The new output should draw a single, solid line over the left lane line and a single, solid line over the right lane line. The lines should start from the bottom of the image and extend out to the top of the region of interest.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the one with the solid yellow lane on the left. This one's more tricky!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video test_videos_output/solidYellowLeft.mp4\n",
      "[MoviePy] Writing video test_videos_output/solidYellowLeft.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████▉| 681/682 [00:21<00:00, 30.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_videos_output/solidYellowLeft.mp4 \n",
      "\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "yellow_output = 'test_videos_output/solidYellowLeft.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "##clip2 = VideoFileClip('test_videos/solidYellowLeft.mp4').subclip(0,5)\n",
    "clip2 = VideoFileClip('test_videos/solidYellowLeft.mp4')\n",
    "yellow_clip = clip2.fl_image(process_image)\n",
    "%time yellow_clip.write_videofile(yellow_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"test_videos_output/solidYellowLeft.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(yellow_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writeup and Submission\n",
    "\n",
    "If you're satisfied with your video outputs, it's time to make the report writeup in a pdf or markdown file. Once you have this Ipython notebook ready along with the writeup, it's time to submit for review! Here is a [link](https://github.com/udacity/CarND-LaneLines-P1/blob/master/writeup_template.md) to the writeup template file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optional Challenge\n",
    "\n",
    "Try your lane finding pipeline on the video below.  Does it still work?  Can you figure out a way to make it more robust?  If you're up for the challenge, modify your pipeline so it works with this video and submit it along with the rest of your project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "challenge_output = 'test_videos_output/challenge.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "##clip3 = VideoFileClip('test_videos/challenge.mp4').subclip(0,5)\n",
    "clip3 = VideoFileClip('test_videos/challenge.mp4')\n",
    "challenge_clip = clip3.fl_image(process_image)\n",
    "%time challenge_clip.write_videofile(challenge_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(challenge_output))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
